# ansible_host is the host to which you connect via ssh.
# internal_host is the host as seen by all the Indexima cluster nodes.
# both are required
[indexima]
node1 ansible_host=localhost is_master=1

[indexima:vars]

##### ANSIBLE VARIABLES #####
#############################

## The user Ansible will use to connect to the machine
ansible_user={{ lookup('env', 'USER') }}
## Absolute path to the ssh private key used to connect to the machine
ansible_ssh_private_key_file=
## Avoid strict host key checking to avoid being stuck
## in elastic environment. More info: https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html#id7
ansible_ssh_extra_args='-o StrictHostKeyChecking=no'
## Use when ansible host
ansible_connection=local
## For password ssh and sudo, install sshpass programm
## apt-get install sshpass / debian based
## yum install -y http://mirror.centos.org/centos/7/extras/x86_64/Packages/sshpass-1.06-2.el7.x86_64.rpm / redhat based
#ansible_ssh_pass=
## WARNING -- If your user doesn't have sudo access without password, you need to specify it here
#ansible_sudo_pass=


##### INDEXIMA GENERAL CONFIG #####
###################################

## Specify service account using Indexima if needed.
## By default, everything will be done as root
#service_user=

## Internet connection : 1 or 0
## Set to 0 if your Indexima nodes don't have external access to internet
## Copy the desired indexima-installer-<version>.zip into roles/indexima-standalone/files
internet=1


## Total number of nodes
nodes=1
## Number of cores per node
#cores=2
## Amount of RAM per node (Mb)
#ram=600
## Number of disk per node
#disk=1
## If you want to set the partitions number manually, uncomment next field and write the desired value
#partitions=4

## Change according to desired version and service pack
#version=1.7.12.1257.0

## Set a custom install path. 'indexima' will automatically be appended to the variable
## Eg. install_path=/opt ; Indexima will be installed in /opt/indexima
## For the prerequistes playbook, 'hadoop' will automatically be appended to the variable
## Eg. install_path=/opt ; Hadoop will be installed in /opt/hadoop
## Default: /opt
#install_path=/opt

## Set a custom logs path.
## Default: /var/log/indexima
#indexima_logs_path=/var/log/indexima

## SSL Galactica
#galactica_ssl=1
#keystore_file=
#keystore_password=

##### WAREHOUSE CONFIG ######
#############################

#### Warehouse type for all nodes (local, nfs, hdfs, azure, s3, gs, or s3like)
### Default: local
#warehouse_type=local

### Local or NFS : Full absolute path to NFS/Local directory used as warehouse
#### Eg. /opt/indexima/indexima-warehouse
### AWS S3 Bucket used : Full S3 path with s3a protocol
#### Eg. s3a://bucket/folder1/folder2/data-folder
### ADLS Bucket used : Full ADL path
#### Eg. adl://bucket/path/data
### GS Bucket used : Full GS path
#### Eg. gs://bucket/path/data
warehouse=/opt/indexima/indexima-warehouse

### Additional infos for fakeS3 Bucket (MiniIO, NetApp,..)
## Specify endpoint and bucket separately
#warehouse_endpoint=http://127.0.0.1:9000
#warehouse_bucket=my_bucket
#s3_like_https=true

##### VD CONFIG #####
#####################

## Choose admin_type between local, s3, azure and gs.
## If choosing local, specify a local absolute path in vd_data to store visualdoop data into
## Defaults: admin_type=local ; vd_data=$HOME/visualdoop-data
#admin_type=local
vd_data=/opt/indexima/visualdoop-data

## If choosing s3, gs or adl, specify vd_bucket, and a relative path to the visualdoop-data object
## Eg. vd_bucket=myawsbucket ; vd_data=path/to/vd ; This will store visualdoop data in s3://myawsbucket/path/to/vd/visualdoop-data
## No Defaults
#vd_bucket=
#vd_data=
#vd_pass=
#admin_users=
#main_connection_user=
#main_connection_password=

## SSL for Dev Console
#vd_ssl=1
#keystore_file=
#keystore_password=


##### CREDENTIALS CONFIG #####
##############################

## AWS // If you need Indexima to use some AWS services, you need to fill the AWS Configs bellow and uncomment the following line
#aws_access_key_id=
#aws_secret_access_key=

## Google Cloud Storage credentials
## GCP // If you need Indexima to use some GCP services (typically Google Storage), you need to uncomment the following line and set the google_credentials_path to the file.
## The credential file must be placed in roles/indexima-standalone/files and named credentials.json
#gcp=1
#google_credentials_path=


##### HIVE CONFIG #####
#######################

## Hive Kerberos
## If you want to configure to use a fully Kerberized Indexima Cluster, uncomment the following line, and specify the kerberos principal and keytab path
## If you want to use only a specific part of Indexima with Kerberos, uncomment only the line(s) corresponding to what you want to Kerberize. Then specify the kerberos principal and keytab path
## WARNING - This playbook assumes keytabs are already present on the machines. It does not manage the keytabs.
#kerberos_full=1
#kerberos_indexima=1
#kerberos_hdfs=1
#kerberos_zookeeper=1
#kerberos_principal=
#kerberos_keytab_path=

## Indexima Hive config
## If you need custom authentication for Indexima Hive, uncomment monitor_auth (If you want to use LDAP, keep commented and see below).
#monitor_auth=1
galactica_users="admin,hadoop"
galactica_passwords="admin,password"
galactica_admins="admin"

##### LDAP CONFIG #####
#######################

## If you want to setup LDAP connection, uncomment the following parameter
#ldap=1
## If LDAP is enabled, you need to configure your ldap url
## Eg. ldap://myldap-server
#ldap_url=
## You also need to configure your user pattern
## Eg. cn=%s,ou=people,dc=indexima,dc=org
#ldap_user_dnpattern=

## LDAP group conf
#ldap_group_dnpattern=
#ldap_group_classkey=groupOfUniqueNames
#ldap_group_membershipkey=uniqueMember
#ldap_group_filter=

## If you need fine rights access to Indexima Monitor, uncomment monitor_rights.
#monitor_rights=1


##### DRIVERS CONFIG #####
##########################

## Zookeeper
#zookeeper_quorum=
#zookeeper_namespace=
is_master=0

## HD Insight
#core_site_path="/etc/hadoop/core-site.xml"
#azure_hadoop_jars_path="/opt/hadoop/share/hadoop/common/"
#fs_adl_oauth2_client_id="00000-0000-00000"
#fs_adl_oauth2_access_token_provider_type="ClientCredential"
#fs_adl_oauth2_tenant_id="11111-1111-11111"
#dfs_adls_oauth2_credential="abcdefg0123="
#fs_defaultFS="adl://example.azuredatalakestore.net"
#fs_adl_impl="org.apache.hadoop.fs.adl.AdlFileSystem"
#fs_AbstractFileSystem_adl_impl="org.apache.hadoop.fs.adl.Adl"